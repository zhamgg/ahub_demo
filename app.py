import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import time
from datetime import datetime, timedelta
import random

# Page configuration
st.set_page_config(
    page_title="AHUB 2.0 - Analytics Hub Demo",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for Great Gray branding
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #1e3a8a 0%, #3b82f6 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #3b82f6;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .status-good {
        color: #10b981;
        font-weight: bold;
    }
    .status-warning {
        color: #f59e0b;
        font-weight: bold;
    }
    .status-error {
        color: #ef4444;
        font-weight: bold;
    }
    .layer-header {
        background: #f8fafc;
        padding: 0.5rem 1rem;
        border-radius: 5px;
        border-left: 4px solid #64748b;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

def generate_sample_data():
    """Generate realistic synthetic data for demo purposes"""
    
    # Data sources simulation
    np.random.seed(42)
    dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')
    
    # Northern Trust data
    nt_data = []
    for i, date in enumerate(dates[:90]):  # Last 90 days
        for account in ['FUND001', 'FUND002', 'FUND003', 'FUND004', 'FUND005']:
            nt_data.append({
                'date': date,
                'account_id': account,
                'nav': 100 + np.random.normal(0, 2) + i * 0.01,
                'total_assets': np.random.uniform(50000000, 200000000),
                'shares_outstanding': np.random.uniform(500000, 2000000),
                'source': 'Northern Trust'
            })
    
    # State Street data
    ss_data = []
    for i, date in enumerate(dates[:90]):
        for account in ['FUND001', 'FUND002', 'FUND003']:
            ss_data.append({
                'date': date,
                'account_id': account,
                'market_value': np.random.uniform(45000000, 190000000),
                'cash_balance': np.random.uniform(1000000, 5000000),
                'accrued_income': np.random.uniform(100000, 500000),
                'source': 'State Street'
            })
    
    # FactSet data
    fs_data = []
    for i, date in enumerate(dates[:90]):
        for account in ['FUND001', 'FUND002', 'FUND004', 'FUND005']:
            fs_data.append({
                'date': date,
                'account_id': account,
                'benchmark_return': np.random.normal(0.0008, 0.02),
                'risk_score': np.random.uniform(1, 10),
                'expense_ratio': np.random.uniform(0.005, 0.025),
                'source': 'FactSet'
            })
    
    return pd.DataFrame(nt_data), pd.DataFrame(ss_data), pd.DataFrame(fs_data)

def create_unified_dataset(nt_df, ss_df, fs_df):
    """Simulate the Silver layer unified data"""
    
    # Create unified fund data
    unified_data = []
    
    for date in nt_df['date'].unique():
        for account in nt_df['account_id'].unique():
            nt_row = nt_df[(nt_df['date'] == date) & (nt_df['account_id'] == account)]
            ss_row = ss_df[(ss_df['date'] == date) & (ss_df['account_id'] == account)]
            fs_row = fs_df[(fs_df['date'] == date) & (fs_df['account_id'] == account)]
            
            if not nt_row.empty:
                unified_row = {
                    'date': date,
                    'fund_id': account,
                    'nav_per_share': nt_row.iloc[0]['nav'],
                    'total_net_assets': nt_row.iloc[0]['total_assets'],
                    'shares_outstanding': nt_row.iloc[0]['shares_outstanding'],
                    'market_value': ss_row.iloc[0]['market_value'] if not ss_row.empty else nt_row.iloc[0]['total_assets'] * 0.95,
                    'cash_balance': ss_row.iloc[0]['cash_balance'] if not ss_row.empty else nt_row.iloc[0]['total_assets'] * 0.05,
                    'benchmark_return': fs_row.iloc[0]['benchmark_return'] if not fs_row.empty else np.random.normal(0.0008, 0.02),
                    'risk_score': fs_row.iloc[0]['risk_score'] if not fs_row.empty else np.random.uniform(1, 10),
                    'data_quality_score': np.random.uniform(85, 99)
                }
                unified_data.append(unified_row)
    
    return pd.DataFrame(unified_data)

def main():
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>ğŸš€ AHUB 2.0 - Analytics Hub Demo</h1>
        <p>From Spreadsheets to AI: Transforming Retirement Technology</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar navigation
    st.sidebar.title("ğŸ§­ Navigation")
    demo_sections = [
        "ğŸ  Overview",
        "ğŸ“¥ Source Data Ingestion", 
        "ğŸ¥‰ Bronze Layer - Raw Staging",
        "ğŸ¥ˆ Silver Layer - Data Vault",
        "ğŸ¥‡ Gold Layer - Business Intelligence",
        "ğŸ“Š Real-time Monitoring",
        "ğŸ’¼ Business Impact"
    ]
    
    selected_section = st.sidebar.selectbox("Select Demo Section:", demo_sections)
    
    # Generate sample data
    if 'data_generated' not in st.session_state:
        with st.spinner("Generating sample data..."):
            st.session_state.nt_data, st.session_state.ss_data, st.session_state.fs_data = generate_sample_data()
            st.session_state.unified_data = create_unified_dataset(
                st.session_state.nt_data, 
                st.session_state.ss_data, 
                st.session_state.fs_data
            )
            st.session_state.data_generated = True
    
    # Section routing
    if selected_section == "ğŸ  Overview":
        show_overview()
    elif selected_section == "ğŸ“¥ Source Data Ingestion":
        show_source_ingestion()
    elif selected_section == "ğŸ¥‰ Bronze Layer - Raw Staging":
        show_bronze_layer()
    elif selected_section == "ğŸ¥ˆ Silver Layer - Data Vault":
        show_silver_layer()
    elif selected_section == "ğŸ¥‡ Gold Layer - Business Intelligence":
        show_gold_layer()
    elif selected_section == "ğŸ“Š Real-time Monitoring":
        show_monitoring()
    elif selected_section == "ğŸ’¼ Business Impact":
        show_business_impact()

def show_overview():
    st.header("ğŸ“‹ AHUB 2.0 Architecture Overview")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ¯ Key Differentiators")
        
        differentiators = [
            ("ğŸ”„ Unified Data", "Single source of truth for all retirement data"),
            ("âš¡ Process Automation", "Intelligent workflows that minimize manual intervention"),
            ("ğŸ”® Predictive Intelligence", "AI that anticipates and prevents issues"),
            ("ğŸ‘¥ Stakeholder Design", "Personalized experiences for each industry stakeholder"),
            ("ğŸ›¡ï¸ Compliance Automation", "Proactive monitoring and reporting"),
            ("ğŸ¤– Self-Optimizing Systems", "Systems that learn and improve autonomously"),
            ("ğŸ”— Seamless Integrations", "Seamless connections to industry platforms")
        ]
        
        for title, description in differentiators:
            with st.container():
                st.markdown(f"**{title}**")
                st.write(description)
                st.markdown("---")
    
    with col2:
        st.subheader("ğŸ“Š Current Status")
        
        # Status metrics
        col_a, col_b = st.columns(2)
        with col_a:
            st.metric("Data Sources", "5", "â†—ï¸ +2")
            st.metric("Data Quality", "96.2%", "â†—ï¸ +12%")
        with col_b:
            st.metric("Processing Speed", "0.88s", "ğŸ”½ -75%")
            st.metric("Automation", "85%", "â†—ï¸ +68%")
        
        st.subheader("ğŸ—ï¸ Architecture Layers")
        layers = ["ğŸ¥‡ Gold - Business Intelligence", "ğŸ¥ˆ Silver - Data Vault", "ğŸ¥‰ Bronze - Raw Staging", "ğŸ“¥ Source Data"]
        
        for i, layer in enumerate(layers):
            if i < len(layers) - 1:
                st.write(f"{layer} â¬‡ï¸")
            else:
                st.write(layer)

def show_source_ingestion():
    st.header("ğŸ“¥ Source Data Ingestion")
    
    st.write("""
    AHUB 2.0 connects to multiple data sources across the retirement ecosystem, 
    automatically identifying and processing new data as it becomes available.
    """)
    
    col1, col2, col3 = st.columns(3)
    
    # Data source simulation
    sources = [
        {"name": "Northern Trust", "status": "âœ… Connected", "last_sync": "2 min ago", "records": 1247},
        {"name": "State Street", "status": "âœ… Connected", "last_sync": "5 min ago", "records": 892},
        {"name": "FactSet", "status": "âœ… Connected", "last_sync": "1 min ago", "records": 1056},
        {"name": "BoardingPass", "status": "âœ… Connected", "last_sync": "3 min ago", "records": 2341},
        {"name": "Morningstar", "status": "â³ Syncing", "last_sync": "Now", "records": 0}
    ]
    
    for i, source in enumerate(sources):
        col = [col1, col2, col3][i % 3]
        with col:
            with st.container():
                st.markdown(f"### {source['name']}")
                st.write(f"**Status:** {source['status']}")
                st.write(f"**Last Sync:** {source['last_sync']}")
                st.write(f"**Records:** {source['records']:,}")
                
                if "Syncing" in source['status']:
                    progress = st.progress(0)
                    for j in range(100):
                        time.sleep(0.01)
                        progress.progress(j + 1)
                    st.success("Sync completed!")
    
    st.subheader("ğŸ“ Sample Raw Data")
    
    # Show sample raw data from different sources
    source_tabs = st.tabs(["Northern Trust", "State Street", "FactSet"])
    
    with source_tabs[0]:
        st.write("**Fund Accounting Valuation Details**")
        sample_nt = st.session_state.nt_data.head(10)[['date', 'account_id', 'nav', 'total_assets', 'shares_outstanding']]
        st.dataframe(sample_nt, use_container_width=True)
    
    with source_tabs[1]:
        st.write("**Custody and Settlement Data**")
        sample_ss = st.session_state.ss_data.head(10)[['date', 'account_id', 'market_value', 'cash_balance', 'accrued_income']]
        st.dataframe(sample_ss, use_container_width=True)
    
    with source_tabs[2]:
        st.write("**Market Data and Analytics**")
        sample_fs = st.session_state.fs_data.head(10)[['date', 'account_id', 'benchmark_return', 'risk_score', 'expense_ratio']]
        st.dataframe(sample_fs, use_container_width=True)

def show_bronze_layer():
    st.header("ğŸ¥‰ Bronze Layer - Raw Data Staging")
    
    st.markdown("""
    <div class="layer-header">
        <h4>ğŸ¯ Purpose: Initial staging and validation of raw data</h4>
    </div>
    """, unsafe_allow_html=True)
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.subheader("ğŸ“Š Staging Tables Overview")
        
        # Simulate staging table stats
        staging_stats = pd.DataFrame({
            'Source': ['Northern Trust', 'State Street', 'FactSet', 'BoardingPass', 'Morningstar'],
            'Records Ingested': [1247, 892, 1056, 2341, 0],
            'Quality Score': [98.2, 96.7, 99.1, 97.4, 0],
            'Processing Time (s)': [0.8, 0.6, 0.9, 1.2, 0],
            'Status': ['âœ… Complete', 'âœ… Complete', 'âœ… Complete', 'âœ… Complete', 'â³ Processing']
        })
        
        st.dataframe(staging_stats, use_container_width=True)
    
    with col2:
        st.subheader("âš¡ Performance")
        st.metric("Avg Processing Time", "0.88s", "ğŸ”½ -75%")
        st.metric("Success Rate", "99.97%", "â†—ï¸ +0.12%")
        st.metric("Data Volume", "5.5GB", "â†—ï¸ +15%")
    
    st.subheader("ğŸ” Data Validation Results")
    
    # Data quality checks
    quality_checks = pd.DataFrame({
        'Check Type': ['Schema Validation', 'Null Value Check', 'Data Type Validation', 'Range Validation', 'Duplicate Detection'],
        'Northern Trust': ['âœ… Pass', 'âœ… Pass', 'âœ… Pass', 'âš ï¸ 2 outliers', 'âœ… Pass'],
        'State Street': ['âœ… Pass', 'âœ… Pass', 'âœ… Pass', 'âœ… Pass', 'âœ… Pass'],
        'FactSet': ['âœ… Pass', 'âœ… Pass', 'âœ… Pass', 'âœ… Pass', 'âœ… Pass'],
        'BoardingPass': ['âœ… Pass', 'âš ï¸ 5 nulls', 'âœ… Pass', 'âœ… Pass', 'âŒ 12 dupes']
    })
    
    st.dataframe(quality_checks, use_container_width=True)
    
    # Show transformation example
    st.subheader("ğŸ”„ Data Transformation Example")
    
    transformation_tabs = st.tabs(["Before", "After", "Changes"])
    
    with transformation_tabs[0]:
        st.write("**Raw Northern Trust Data**")
        raw_sample = pd.DataFrame({
            'Account ID (FA)': ['FUND001', 'FUND002'],
            'Total MV Securities (Base)': ['$50,000,000.00', '$75,000,000.00'],
            'NAV (8 Precision)': ['100.12345678', '98.87654321'],
            'As of Date': ['2024-01-15', '2024-01-15']
        })
        st.dataframe(raw_sample, use_container_width=True)
    
    with transformation_tabs[1]:
        st.write("**Cleaned Staging Data**")
        clean_sample = pd.DataFrame({
            'account_id': ['FUND001', 'FUND002'],
            'total_market_value': [50000000.00, 75000000.00],
            'nav_per_share': [100.12345678, 98.87654321],
            'valuation_date': ['2024-01-15', '2024-01-15'],
            'source_system': ['Northern Trust', 'Northern Trust'],
            'ingestion_timestamp': ['2024-01-15 09:30:00', '2024-01-15 09:30:00']
        })
        st.dataframe(clean_sample, use_container_width=True)
    
    with transformation_tabs[2]:
        st.write("**Applied Transformations:**")
        st.write("âœ… Removed special characters from column names")
        st.write("âœ… Converted currency strings to numeric values")
        st.write("âœ… Standardized date formats")
        st.write("âœ… Added audit fields (source_system, ingestion_timestamp)")
        st.write("âœ… Applied data type validations")

def show_silver_layer():
    st.header("ğŸ¥ˆ Silver Layer - Data Vault")
    
    st.markdown("""
    <div class="layer-header">
        <h4>ğŸ¯ Purpose: Unified data integration and business rule application</h4>
    </div>
    """, unsafe_allow_html=True)
    
    # Show the medallion architecture progress
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("### ğŸ¥‰ Bronze â†’ ğŸ¥ˆ Silver")
        st.write("âœ… Raw data vault created")
        st.write("âœ… Business rules applied")
        st.write("âœ… Data quality improved")
    
    with col2:
        st.markdown("### ğŸ“Š Integration Stats")
        st.metric("Sources Unified", "5", "â†—ï¸ +2")
        st.metric("Data Quality", "96.2%", "â†—ï¸ +12%")
        st.metric("Records Matched", "98.7%", "â†—ï¸ +8%")
    
    with col3:
        st.markdown("### âš™ï¸ Business Rules")
        st.write("âœ… Master Data Management")
        st.write("âœ… Cross-source validation")
        st.write("âœ… Calculated fields added")
    
    st.subheader("ğŸ”— Data Vault Model")
    
    # Show unified data sample
    st.write("**Unified Fund Data (Business Data Vault)**")
    unified_sample = st.session_state.unified_data.head(10)
    st.dataframe(unified_sample, use_container_width=True)
    
    # Data lineage visualization
    st.subheader("ğŸ” Data Lineage & Integration")
    
    lineage_data = pd.DataFrame({
        'Fund ID': ['FUND001', 'FUND001', 'FUND002', 'FUND002'],
        'Source System': ['Northern Trust', 'State Street', 'Northern Trust', 'FactSet'],
        'Data Element': ['NAV', 'Market Value', 'NAV', 'Benchmark Return'],
        'Last Updated': ['2024-01-15 09:30', '2024-01-15 09:32', '2024-01-15 09:30', '2024-01-15 09:28'],
        'Quality Score': [99.2, 97.8, 98.9, 99.5]
    })
    
    st.dataframe(lineage_data, use_container_width=True)
    
    # Show data quality improvements
    st.subheader("ğŸ“ˆ Data Quality Improvements")
    
    # Create before/after quality comparison
    quality_comparison = pd.DataFrame({
        'Metric': ['Completeness', 'Accuracy', 'Consistency', 'Timeliness', 'Validity'],
        'Before (Bronze)': [87.2, 91.5, 78.3, 85.7, 89.1],
        'After (Silver)': [96.8, 98.2, 95.4, 97.1, 98.9],
        'Improvement': ['+9.6%', '+6.7%', '+17.1%', '+11.4%', '+9.8%']
    })
    
    fig = px.bar(quality_comparison, x='Metric', y=['Before (Bronze)', 'After (Silver)'], 
                 title="Data Quality: Bronze vs Silver Layer",
                 barmode='group',
                 color_discrete_map={'Before (Bronze)': '#ef4444', 'After (Silver)': '#10b981'})
    
    st.plotly_chart(fig, use_container_width=True)

def show_gold_layer():
    st.header("ğŸ¥‡ Gold Layer - Business Intelligence")
    
    st.markdown("""
    <div class="layer-header">
        <h4>ğŸ¯ Purpose: Business-ready data for analytics and reporting</h4>
    </div>
    """, unsafe_allow_html=True)
    
    # Key metrics dashboard
    st.subheader("ğŸ“Š Executive Dashboard")
    
    col1, col2, col3, col4 = st.columns(4)
    
    # Calculate some business metrics
    total_aum = st.session_state.unified_data['total_net_assets'].sum() / 1e9
    avg_nav = st.session_state.unified_data['nav_per_share'].mean()
    avg_risk = st.session_state.unified_data['risk_score'].mean()
    quality_score = st.session_state.unified_data['data_quality_score'].mean()
    
    with col1:
        st.metric("Total AUM", f"${total_aum:.1f}B", "â†—ï¸ +5.2%")
    with col2:
        st.metric("Avg NAV", f"${avg_nav:.2f}", "â†—ï¸ +0.8%")
    with col3:
        st.metric("Avg Risk Score", f"{avg_risk:.1f}/10", "ğŸ”½ -0.3")
    with col4:
        st.metric("Data Quality", f"{quality_score:.1f}%", "â†—ï¸ +2.1%")
    
    # Fund performance visualization
    st.subheader("ğŸ“ˆ Fund Performance Analysis")
    
    # Create performance chart
    daily_performance = st.session_state.unified_data.groupby(['date', 'fund_id']).agg({
        'nav_per_share': 'mean',
        'total_net_assets': 'sum'
    }).reset_index()
    
    fig = px.line(daily_performance, x='date', y='nav_per_share', color='fund_id',
                  title="NAV Performance by Fund",
                  labels={'nav_per_share': 'NAV per Share ($)', 'date': 'Date'})
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Risk analysis
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ¯ Risk Distribution")
        risk_dist = st.session_state.unified_data.groupby('fund_id')['risk_score'].mean().reset_index()
        
        fig_risk = px.bar(risk_dist, x='fund_id', y='risk_score',
                         title="Average Risk Score by Fund",
                         color='risk_score',
                         color_continuous_scale='RdYlGn_r')
        
        st.plotly_chart(fig_risk, use_container_width=True)
    
    with col2:
        st.subheader("ğŸ’° Asset Allocation")
        asset_allocation = st.session_state.unified_data.groupby('fund_id')['total_net_assets'].sum().reset_index()
        
        fig_assets = px.pie(asset_allocation, values='total_net_assets', names='fund_id',
                           title="AUM Distribution by Fund")
        
        st.plotly_chart(fig_assets, use_container_width=True)
    
    # Automated reports section
    st.subheader("ğŸ“„ Automated Reports")
    
    report_tabs = st.tabs(["Daily Summary", "Risk Report", "Compliance Check"])
    
    with report_tabs[0]:
        st.write("**Daily Fund Summary Report - Generated Automatically**")
        daily_summary = st.session_state.unified_data.groupby('fund_id').agg({
            'nav_per_share': ['mean', 'std'],
            'total_net_assets': 'sum',
            'risk_score': 'mean',
            'data_quality_score': 'mean'
        }).round(2)
        
        daily_summary.columns = ['Avg NAV', 'NAV Volatility', 'Total Assets', 'Risk Score', 'Data Quality']
        st.dataframe(daily_summary, use_container_width=True)
    
    with report_tabs[1]:
        st.write("**Risk Monitoring Report**")
        
        # Risk alerts
        high_risk_funds = st.session_state.unified_data[st.session_state.unified_data['risk_score'] > 7]
        if not high_risk_funds.empty:
            st.warning(f"âš ï¸ {len(high_risk_funds)} funds showing elevated risk levels")
            st.dataframe(high_risk_funds[['fund_id', 'risk_score', 'date']].head(), use_container_width=True)
        else:
            st.success("âœ… All funds within acceptable risk parameters")
    
    with report_tabs[2]:
        st.write("**Compliance Monitoring**")
        
        compliance_checks = pd.DataFrame({
            'Check': ['Data Completeness', 'Regulatory Reporting', 'NAV Validation', 'Risk Limits', 'Documentation'],
            'Status': ['âœ… Pass', 'âœ… Pass', 'âœ… Pass', 'âš ï¸ Review', 'âœ… Pass'],
            'Last Check': ['09:30 AM', '09:28 AM', '09:31 AM', '09:29 AM', '09:27 AM']
        })
        
        st.dataframe(compliance_checks, use_container_width=True)

def show_monitoring():
    st.header("ğŸ“Š Real-time Monitoring & Alerts")
    
    st.markdown("""
    <div class="layer-header">
        <h4>ğŸ¯ Purpose: Proactive monitoring and self-optimizing systems</h4>
    </div>
    """, unsafe_allow_html=True)
    
    # System health metrics
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.subheader("ğŸ–¥ï¸ System Health")
        st.metric("Uptime", "99.97%", "â†—ï¸ +0.02%")
        st.metric("Response Time", "0.88s", "ğŸ”½ -0.45s")
        st.metric("Throughput", "1.2M rec/hr", "â†—ï¸ +15%")
    
    with col2:
        st.subheader("ğŸ”„ Pipeline Status")
        st.metric("Active Pipelines", "12", "â†’ 0")
        st.metric("Success Rate", "99.8%", "â†—ï¸ +0.3%")
        st.metric("Avg Processing", "2.1 min", "ğŸ”½ -30s")
    
    with col3:
        st.subheader("ğŸ›¡ï¸ Security & Compliance")
        st.metric("Access Violations", "0", "â†’ 0")
        st.metric("Data Leakage Alerts", "0", "â†’ 0")
        st.metric("Compliance Score", "98.5%", "â†—ï¸ +1.2%")
    
    # Real-time alerts
    st.subheader("ğŸš¨ Active Alerts & Notifications")
    
    # Simulate real-time alerts
    alerts_data = pd.DataFrame({
        'Time': ['09:31:23', '09:28:45', '09:25:12', '09:22:33'],
        'Type': ['INFO', 'WARNING', 'SUCCESS', 'INFO'],
        'Component': ['Data Quality', 'Processing', 'Integration', 'Security'],
        'Message': [
            'FUND004 data quality score improved to 99.2%',
            'Processing time for State Street feed increased by 15%',
            'FactSet integration completed successfully - 1,056 records',
            'Daily security scan completed - no issues found'
        ],
        'Action': ['None', 'Monitor', 'None', 'None']
    })
    
    # Color code alerts
    def style_alerts(row):
        if row['Type'] == 'WARNING':
            return ['background-color: #fef3c7'] * len(row)
        elif row['Type'] == 'SUCCESS':
            return ['background-color: #d1fae5'] * len(row)
        else:
            return ['background-color: #f0f9ff'] * len(row)
    
    styled_alerts = alerts_data.style.apply(style_alerts, axis=1)
    st.dataframe(styled_alerts, use_container_width=True)
    
    # Data quality monitoring
    st.subheader("ğŸ“ˆ Data Quality Trends")
    
    # Generate quality trend data
    dates = pd.date_range(start='2024-01-01', end='2024-01-15', freq='D')
    quality_trends = pd.DataFrame({
        'Date': dates,
        'Overall Quality': [85 + i + np.random.normal(0, 2) for i in range(len(dates))],
        'Completeness': [88 + i * 0.5 + np.random.normal(0, 1) for i in range(len(dates))],
        'Accuracy': [90 + i * 0.3 + np.random.normal(0, 1.5) for i in range(len(dates))],
        'Timeliness': [85 + i * 0.8 + np.random.normal(0, 1) for i in range(len(dates))]
    })
    
    fig_quality = px.line(quality_trends, x='Date', y=['Overall Quality', 'Completeness', 'Accuracy', 'Timeliness'],
                         title="Data Quality Trends (Last 15 Days)",
                         labels={'value': 'Quality Score (%)', 'variable': 'Metric'})
    
    st.plotly_chart(fig_quality, use_container_width=True)
    
    # Predictive alerts
    st.subheader("ğŸ”® Predictive Intelligence")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Predicted Issues (Next 24 Hours)**")
        predictions = pd.DataFrame({
            'Risk Level': ['ğŸŸ¡ Medium', 'ğŸŸ¢ Low', 'ğŸŸ¡ Medium', 'ğŸŸ¢ Low'],
            'Component': ['State Street Feed', 'Data Quality', 'Processing Load', 'Storage Capacity'],
            'Prediction': [
                'Processing time may increase by 20%',
                'Quality scores expected to remain stable',
                'Peak load expected at 2 PM EST',
                'Storage utilization normal'
            ],
            'Confidence': ['85%', '92%', '78%', '94%']
        })
        st.dataframe(predictions, use_container_width=True)
    
    with col2:
        st.write("**Auto-Optimization Actions**")
        optimizations = pd.DataFrame({
            'Time': ['09:15', '09:20', '09:25', '09:30'],
            'Action': [
                'Increased processing capacity for peak load',
                'Optimized query execution plan',
                'Adjusted data quality thresholds',
                'Balanced workload across clusters'
            ],
            'Impact': ['+15% throughput', '-0.3s latency', '+2% accuracy', '+8% efficiency']
        })
        st.dataframe(optimizations, use_container_width=True)

def show_business_impact():
    st.header("ğŸ’¼ Business Impact & ROI")
    
    st.markdown("""
    <div class="layer-header">
        <h4>ğŸ¯ Purpose: Demonstrating tangible business value and competitive advantage</h4>
    </div>
    """, unsafe_allow_html=True)
    
    # ROI Metrics
    st.subheader("ğŸ’° Return on Investment")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Time Savings", "75%", "â†—ï¸ Manual â†’ Auto")
        st.write("*Process automation*")
    
    with col2:
        st.metric("Error Reduction", "88%", "ğŸ”½ Data inconsistencies")
        st.write("*Quality improvements*")
    
    with col3:
        st.metric("Faster Insights", "10x", "â†—ï¸ Hours â†’ Minutes")
        st.write("*Real-time analytics*")
    
    with col4:
        st.metric("Cost Reduction", "$2.1M", "ğŸ”½ Annual savings")
        st.write("*Operational efficiency*")
    
    # Before vs After comparison
    st.subheader("ğŸ“Š Before vs After AHUB 2.0")
    
    comparison_data = pd.DataFrame({
        'Metric': [
            'Report Generation Time',
            'Data Quality Issues',
            'Manual Interventions',
            'Data Latency',
            'Compliance Violations',
            'System Downtime'
        ],
        'Before AHUB 2.0': ['4-6 hours', '15-20 per week', '40+ per day', '2-4 hours', '2-3 per month', '8 hours/month'],
        'After AHUB 2.0': ['5-10 minutes', '1-2 per week', '3-5 per day', '< 5 minutes', '0 per month', '< 1 hour/month'],
        'Improvement': ['95% faster', '90% reduction', '87% reduction', '95% faster', '100% reduction', '87% reduction']
    })
    
    st.dataframe(comparison_data, use_container_width=True)
    
    # Stakeholder benefits
    st.subheader("ğŸ‘¥ Stakeholder Benefits")
    
    stakeholder_tabs = st.tabs(["Plan Sponsors", "Advisors", "Participants", "Operations"])
    
    with stakeholder_tabs[0]:
        st.write("**Plan Sponsors**")
        benefits = [
            "ğŸ” **Real-time visibility** into plan performance and health",
            "ğŸ“Š **Automated compliance** reporting and monitoring", 
            "ğŸ’° **Cost reduction** through operational efficiency",
            "ğŸ›¡ï¸ **Risk mitigation** through predictive analytics",
            "ğŸ“ˆ **Better outcomes** for plan participants"
        ]
        for benefit in benefits:
            st.markdown(benefit)
    
    with stakeholder_tabs[1]:
        st.write("**Advisors**")
        benefits = [
            "âš¡ **Faster proposal** generation (hours â†’ minutes)",
            "ğŸ¯ **Personalized recommendations** based on comprehensive data",
            "ğŸ“Š **Enhanced client** presentations with real-time data",
            "ğŸ”„ **Automated follow-up** scheduling and reminders",
            "ğŸ’¡ **Predictive insights** for proactive client management"
        ]
        for benefit in benefits:
            st.markdown(benefit)
    
    with stakeholder_tabs[2]:
        st.write("**Plan Participants**")
        benefits = [
            "ğŸ“± **Real-time account** information and updates",
            "ğŸ¯ **Personalized retirement** planning recommendations",
            "ğŸ“Š **Clear, visual** performance tracking",
            "ğŸ”” **Proactive notifications** about important changes",
            "ğŸ’¼ **Seamless experience** across all touchpoints"
        ]
        for benefit in benefits:
            st.markdown(benefit)
    
    with stakeholder_tabs[3]:
        st.write("**Operations Teams**")
        benefits = [
            "ğŸ¤– **85% automation** of manual processes",
            "ğŸ” **Proactive issue** detection and resolution",
            "ğŸ“ˆ **Improved data quality** and consistency",
            "âš¡ **Faster processing** and reduced latency",
            "ğŸ›¡ï¸ **Enhanced security** and compliance monitoring"
        ]
        for benefit in benefits:
            st.markdown(benefit)
    
    # Competitive advantages
    st.subheader("ğŸ† Competitive Advantages")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Industry Challenges AHUB 2.0 Solves:**")
        challenges = [
            "âŒ Siloed data across multiple systems",
            "âŒ Manual, error-prone processes", 
            "âŒ Delayed reporting and insights",
            "âŒ Compliance and regulatory challenges",
            "âŒ Poor data quality and consistency",
            "âŒ Reactive vs. proactive management"
        ]
        for challenge in challenges:
            st.write(challenge)
    
    with col2:
        st.write("**Our Solution:**")
        solutions = [
            "âœ… Unified data platform - single source of truth",
            "âœ… AI-powered automation and optimization",
            "âœ… Real-time insights and predictive analytics", 
            "âœ… Automated compliance monitoring",
            "âœ… Statistical process control for data quality",
            "âœ… Predictive intelligence prevents issues"
        ]
        for solution in solutions:
            st.write(solution)
    
    # Future roadmap
    st.subheader("ğŸš€ Future Roadmap")
    
    roadmap_data = pd.DataFrame({
        'Quarter': ['Q2 2025', 'Q3 2025', 'Q4 2025', 'Q1 2026'],
        'Feature': [
            'Advanced AI Workflows',
            'Predictive Participant Behavior',
            'Natural Language Queries', 
            'Full Ecosystem Integration'
        ],
        'Impact': [
            'Further automation of complex processes',
            'Personalized participant engagement',
            'Self-service analytics for all users',
            'Seamless data flow across all products'
        ]
    })
    
    st.dataframe(roadmap_data, use_container_width=True)
    
    # Call to action
    st.markdown("""
    ---
    ### ğŸ¯ Ready to Transform Your Retirement Technology?
    
    **AHUB 2.0 represents the future of retirement data management** - moving from reactive spreadsheets 
    to proactive, AI-powered intelligence that drives better outcomes for everyone in the ecosystem.
    
    **Contact us to learn more about implementing AHUB 2.0 for your organization.**
    """)

if __name__ == "__main__":
    main()
